# TALLERIA3CORTE
# Aprendizaje por Refuerzo y Algoritmos Bayesianos

## 1. Aprendizaje por Refuerzo (Reinforcement Learning - RL)

El aprendizaje por refuerzo es un paradigma de la inteligencia artificial donde un agente aprende a tomar decisiones mediante la interacci√≥n con un entorno, recibiendo recompensas o castigos seg√∫n sus acciones.

### 1.a ¬øC√≥mo puede un agente aprender a tomar decisiones √≥ptimas en un entorno incierto?

Un agente aprende mediante un proceso de prueba y error, siguiendo estos principios:

#### 1. Interacci√≥n continua
El agente realiza acciones dentro del entorno y recibe:
- Un estado (informaci√≥n del entorno)
- Una recompensa (resultado positivo o negativo)
- El nuevo estado tras la acci√≥n

#### 2. Funci√≥n de recompensa
El agente ajusta su comportamiento buscando maximizar la recompensa acumulada a largo plazo.

#### 3. Exploraci√≥n vs. explotaci√≥n
En entornos inciertos, el agente debe:
- **Explorar**: probar acciones nuevas para descubrir mejores resultados
- **Explotar**: usar las acciones que ya sabe que dan buenas recompensas

#### 4. Evaluaci√≥n de pol√≠ticas
El agente aprende una pol√≠tica √≥ptima, es decir, un conjunto de reglas para elegir la acci√≥n √≥ptima en cada estado.

#### 5. Convergencia
Con suficientes iteraciones y retroalimentaci√≥n, el agente converge a decisiones √≥ptimas incluso en escenarios inciertos.

### 1.b Tipos de algoritmos de aprendizaje por refuerzo y sus arquitecturas

Los algoritmos de RL se dividen en tres categor√≠as principales:

#### üéØ 1. M√©todos basados en valores (Value-Based)
El agente aprende una funci√≥n de valor, que estima qu√© tan buena es una acci√≥n en un estado.

**Ejemplos:**
- Q-Learning
- Deep Q-Network (DQN)

**Arquitectura:**
- Estados (S)
- Acciones (A)
- Q-Table o red neuronal (para DQN)
- Pol√≠tica Œµ-greedy
- Recompensa (R)

#### üéØ 2. M√©todos basados en pol√≠ticas (Policy-Based)
El agente aprende directamente una pol√≠tica que asigna probabilidades a cada acci√≥n.

**Ejemplos:**
- REINFORCE
- Policy Gradient Methods

**Arquitectura:**
- Estados
- Red neuronal para la pol√≠tica œÄ(a|s)
- Funci√≥n de p√©rdida basada en recompensa
- Actualizaci√≥n por gradiente

#### üéØ 3. M√©todos actor-cr√≠tico (Actor-Critic)
Combinan los dos anteriores:
- **Actor**: decide la acci√≥n
- **Cr√≠tico**: eval√∫a la calidad de la acci√≥n tomada

**Ejemplos:**
- A2C (Advantage Actor-Critic)
- A3C (Asynchronous Advantage Actor-Critic)
- PPO (Proximal Policy Optimization)

**Arquitectura:**
- Red neuronal compartida o dos redes:
  - Actor: pol√≠tica
  - Cr√≠tico: valor (V) o ventaja (A)
- Funci√≥n de p√©rdida h√≠brida

### 1.c ¬øPara qu√© se utilizan estos algoritmos en la industria?

#### Industria 4.0
- Optimizaci√≥n de procesos productivos
- Control inteligente de robots
- Sistemas de control aut√≥nomos
- Predicci√≥n de mantenimiento (mantenimiento predictivo)

#### Telecomunicaciones
- Optimizaci√≥n din√°mica de redes
- Gesti√≥n de tr√°fico
- Selecci√≥n autom√°tica de rutas √≥ptimas en routers
- Asignaci√≥n de espectro en redes 5G

#### Finanzas
- Trading algor√≠tmico
- Evaluaci√≥n de riesgo

#### Transporte
- Veh√≠culos aut√≥nomos
- Ruteo inteligente

#### Energ√≠a
- Optimizaci√≥n de redes el√©ctricas
- Control de consumo energ√©tico en edificios

## 2. Algoritmo bayesiano para clasificaci√≥n de SPAM

Se desea calcular:  
**P(Spam‚à£gratis)**

### Datos:
- P(Spam) = 0.3
- P(¬¨Spam) = 0.7
- P(gratis | Spam) = 0.8
- P(gratis | ¬¨Spam) = 0.1

### Soluci√≥n usando el Teorema de Bayes:

\[
P(Spam‚à£gratis) = \frac{P(gratis‚à£Spam) \cdot P(Spam)}{P(gratis)}
\]

#### Primero calculamos P(gratis):
\[
\begin{align*}
P(gratis) &= P(gratis‚à£Spam)P(Spam) + P(gratis‚à£¬¨Spam)P(¬¨Spam) \\
&= 0.8 \times 0.3 + 0.1 \times 0.7 \\
&= 0.24 + 0.07 = 0.31
\end{align*}
\]

#### Ahora calculamos la probabilidad final:
\[
\begin{align*}
P(Spam‚à£gratis) &= \frac{0.8 \times 0.3}{0.31} \\
&= \frac{0.24}{0.31} \approx 0.774
\end{align*}
\]

### Algoritmo simple en pseudoc√≥digo

```python
function prob_spam(p_spam, p_not_spam, p_gratis_spam, p_gratis_not_spam):
    p_gratis = p_gratis_spam * p_spam + p_gratis_not_spam * p_not_spam
    result = (p_gratis_spam * p_spam) / p_gratis
    return result

# Datos del problema
p_spam = 0.3
p_not_spam = 0.7
p_gratis_spam = 0.8
p_gratis_not_spam = 0.1

print(prob_spam(p_spam, p_not_spam, p_gratis_spam, p_gratis_not_spam))
Resultado:
üëâ ‚âà 0.774 ‚Üí 77.4% de probabilidad de que sea SPAM

3. Algoritmos m√°s utilizados actualmente y sus caracter√≠sticas
Algoritmo	Tipo	Uso Principal	Caracter√≠sticas
Deep Neural Networks (DNN)	Supervisado	Clasificaci√≥n y regresi√≥n general	Profundas, usan backprop; alta precisi√≥n
Convolutional Neural Networks (CNN)	Supervisado	Visi√≥n artificial	Extraen caracter√≠sticas espaciales; usadas en im√°genes, CCTV, rob√≥tica
Recurrent Neural Networks (RNN)	Supervisado	Series de tiempo, texto	Modelo secuencial con memoria
LSTM / GRU	Supervisado	Modelos temporales avanzados	Resuelven desvanecimiento de gradiente
Transformers	Supervisado / autosupervisado	NLP e IA modernas (GPT, BERT)	Atenci√≥n multi-cabeza, paralelizaci√≥n
Random Forest	Supervisado	Clasificaci√≥n tabular	Ensamble de √°rboles, robusto, f√°cil entrenamiento
Gradient Boosting (XGBoost, LightGBM)	Supervisado	Tablas de datos con alta precisi√≥n	Altamente usado en industria
K-Means	No supervisado	Clustering	Simple y r√°pido para segmentaci√≥n
DBSCAN	No supervisado	Detecci√≥n de anomal√≠as	Robusto a ruido
Aprendizaje por refuerzo (RL)	RL	Sistemas aut√≥nomos	Optimiza decisiones secuenciales
SVM	Supervisado	Datos de alta dimensi√≥n	Clasificaci√≥n eficiente con m√°rgenes
Naive Bayes	Supervisado	Clasificaci√≥n simple (ej. SPAM)	R√°pido, probabil√≠stico
